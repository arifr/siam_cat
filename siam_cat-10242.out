Currently using GPU: 0
Creating dataset: SOPClass
create labels..
Creating model: mobilenet
MobileNet(
  (base_model): MobileNetV3(
    (features): Sequential(
      (0): ConvBNActivation(
        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
      (1): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (2): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (3): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (4): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (5): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (6): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (7): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (8): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (9): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (10): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (11): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (12): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (13): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (14): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (15): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (16): ConvBNActivation(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (classifier): Sequential(
      (0): Linear(in_features=960, out_features=1280, bias=True)
      (1): Hardswish()
      (2): Dropout(p=0.2, inplace=True)
      (3): Linear(in_features=1280, out_features=12, bias=True)
    )
  )
)
Linear(in_features=1280, out_features=64, bias=True)
file name: mg_catsiam_MobileNetV3_SOPClass_96_100_pairmargin_pair_0.0001_64
>> Epoch 1/100
acc: 64.820/64.820 loss: 0.930
>> Epoch 2/100
acc: 73.644/73.644 loss: 0.585
>> Epoch 3/100
acc: 76.712/76.712 loss: 0.496
>> Epoch 4/100
acc: 78.697/78.697 loss: 0.442
>> Epoch 5/100
acc: 79.982/79.982 loss: 0.406
>> Epoch 6/100
acc: 81.147/81.147 loss: 0.371
>> Epoch 7/100
acc: 82.516/82.516 loss: 0.344
>> Epoch 8/100
acc: 83.340/83.340 loss: 0.322
>> Epoch 9/100
acc: 84.064/84.064 loss: 0.305
>> Epoch 10/100
acc: 84.973/84.973 loss: 0.289
>> Epoch 11/100
acc: 85.517/85.517 loss: 0.272
>> Epoch 12/100
acc: 86.286/86.286 loss: 0.260
>> Epoch 13/100
acc: 86.706/86.706 loss: 0.247
>> Epoch 14/100
acc: 87.293/87.293 loss: 0.236
>> Epoch 15/100
acc: 87.703/87.703 loss: 0.232
>> Epoch 16/100
acc: 88.213/88.213 loss: 0.221
>> Epoch 17/100
acc: 88.338/88.338 loss: 0.217
>> Epoch 18/100
acc: 88.934/88.934 loss: 0.206
>> Epoch 19/100
acc: 89.182/89.182 loss: 0.202
>> Epoch 20/100
acc: 89.626/89.626 loss: 0.194
>> Epoch 21/100
acc: 90.900/90.900 loss: 0.166
>> Epoch 22/100
acc: 91.021/91.021 loss: 0.167
>> Epoch 23/100
acc: 91.481/91.481 loss: 0.158
>> Epoch 24/100
acc: 91.475/91.481 loss: 0.157
>> Epoch 25/100
acc: 91.725/91.725 loss: 0.152
>> Epoch 26/100
acc: 91.841/91.841 loss: 0.152
>> Epoch 27/100
acc: 92.121/92.121 loss: 0.149
>> Epoch 28/100
acc: 92.198/92.198 loss: 0.145
>> Epoch 29/100
acc: 92.212/92.212 loss: 0.142
>> Epoch 30/100
acc: 92.237/92.237 loss: 0.143
>> Epoch 31/100
acc: 92.418/92.418 loss: 0.142
>> Epoch 32/100
acc: 92.497/92.497 loss: 0.137
>> Epoch 33/100
acc: 92.606/92.606 loss: 0.137
>> Epoch 34/100
acc: 92.726/92.726 loss: 0.136
>> Epoch 35/100
acc: 92.939/92.939 loss: 0.133
>> Epoch 36/100
acc: 92.915/92.939 loss: 0.131
>> Epoch 37/100
acc: 93.083/93.083 loss: 0.129
>> Epoch 38/100
acc: 93.179/93.179 loss: 0.127
>> Epoch 39/100
acc: 93.229/93.229 loss: 0.125
>> Epoch 40/100
acc: 93.387/93.387 loss: 0.122
>> Epoch 41/100
acc: 93.787/93.787 loss: 0.118
>> Epoch 42/100
acc: 93.920/93.920 loss: 0.115
>> Epoch 43/100
acc: 94.239/94.239 loss: 0.111
>> Epoch 44/100
acc: 94.035/94.239 loss: 0.115
>> Epoch 45/100
acc: 94.045/94.239 loss: 0.107
>> Epoch 46/100
acc: 94.247/94.247 loss: 0.109
>> Epoch 47/100
acc: 94.391/94.391 loss: 0.104
>> Epoch 48/100
acc: 94.242/94.391 loss: 0.107
>> Epoch 49/100
acc: 94.307/94.391 loss: 0.104
>> Epoch 50/100
acc: 94.405/94.405 loss: 0.102
>> Epoch 51/100
acc: 94.507/94.507 loss: 0.103
>> Epoch 52/100
acc: 94.470/94.507 loss: 0.102
>> Epoch 53/100
acc: 94.548/94.548 loss: 0.100
>> Epoch 54/100
acc: 94.684/94.684 loss: 0.101
>> Epoch 55/100
acc: 94.544/94.684 loss: 0.104
>> Epoch 56/100
acc: 94.532/94.684 loss: 0.101
>> Epoch 57/100
acc: 94.657/94.684 loss: 0.100
>> Epoch 58/100
acc: 94.734/94.734 loss: 0.098
>> Epoch 59/100
acc: 94.675/94.734 loss: 0.102
>> Epoch 60/100
acc: 94.643/94.734 loss: 0.099
>> Epoch 61/100
acc: 94.811/94.811 loss: 0.096
>> Epoch 62/100
acc: 94.910/94.910 loss: 0.097
>> Epoch 63/100
acc: 95.159/95.159 loss: 0.091
>> Epoch 64/100
acc: 95.145/95.159 loss: 0.091
>> Epoch 65/100
acc: 95.045/95.159 loss: 0.094
>> Epoch 66/100
acc: 95.132/95.159 loss: 0.093
>> Epoch 67/100
acc: 95.108/95.159 loss: 0.093
>> Epoch 68/100
acc: 95.251/95.251 loss: 0.090
>> Epoch 69/100
acc: 95.209/95.251 loss: 0.090
>> Epoch 70/100
acc: 95.355/95.355 loss: 0.090
>> Epoch 71/100
acc: 95.295/95.355 loss: 0.087
>> Epoch 72/100
acc: 95.181/95.355 loss: 0.091
>> Epoch 73/100
acc: 95.261/95.355 loss: 0.088
>> Epoch 74/100
acc: 95.317/95.355 loss: 0.088
>> Epoch 75/100
acc: 95.139/95.355 loss: 0.090
>> Epoch 76/100
acc: 95.342/95.355 loss: 0.087
>> Epoch 77/100
acc: 95.253/95.355 loss: 0.089
>> Epoch 78/100
acc: 95.169/95.355 loss: 0.090
>> Epoch 79/100
acc: 95.443/95.443 loss: 0.087
>> Epoch 80/100
acc: 95.349/95.443 loss: 0.089
>> Epoch 81/100
acc: 95.510/95.510 loss: 0.083
>> Epoch 82/100
acc: 95.594/95.594 loss: 0.084
>> Epoch 83/100
acc: 95.510/95.594 loss: 0.083
>> Epoch 84/100
acc: 95.466/95.594 loss: 0.084
>> Epoch 85/100
acc: 95.508/95.594 loss: 0.084
>> Epoch 86/100
acc: 95.446/95.594 loss: 0.086
>> Epoch 87/100
acc: 95.658/95.658 loss: 0.080
>> Epoch 88/100
acc: 95.741/95.741 loss: 0.080
>> Epoch 89/100
acc: 95.629/95.741 loss: 0.082
>> Epoch 90/100
acc: 95.616/95.741 loss: 0.080
>> Epoch 91/100
acc: 95.619/95.741 loss: 0.083
>> Epoch 92/100
acc: 95.629/95.741 loss: 0.085
>> Epoch 93/100
acc: 95.622/95.741 loss: 0.081
>> Epoch 94/100
acc: 95.673/95.741 loss: 0.082
>> Epoch 95/100
acc: 95.637/95.741 loss: 0.083
>> Epoch 96/100
acc: 95.752/95.752 loss: 0.080
>> Epoch 97/100
acc: 95.819/95.819 loss: 0.080
>> Epoch 98/100
acc: 95.710/95.819 loss: 0.081
>> Epoch 99/100
acc: 95.830/95.830 loss: 0.077
>> Epoch 100/100
acc: 95.715/95.830 loss: 0.080
mg_catsiam_MobileNetV3_SOPClass_96_100_pairmargin_pair_0.0001_64  saved.
mg_catsiam_emb_MobileNetV3_SOPClass_96_100_pairmargin_pair_0.0001_64  saved.
== Validation ==

Accuracy (%): 95.8304672241211	 Error rate (%): 4.169532775878906
Finished. Total elapsed time (h:m:s): 7:46:17
Currently using GPU: 0
Creating dataset: SOPClass
create labels..
Creating model: mobilenet
MobileNet(
  (base_model): MobileNetV3(
    (features): Sequential(
      (0): ConvBNActivation(
        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
      (1): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (2): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (3): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (4): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (5): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (6): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (7): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (8): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (9): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (10): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (11): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (12): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (13): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (14): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (15): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (16): ConvBNActivation(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (classifier): Sequential(
      (0): Linear(in_features=960, out_features=1280, bias=True)
      (1): Hardswish()
      (2): Dropout(p=0.2, inplace=True)
      (3): Linear(in_features=1280, out_features=12, bias=True)
    )
  )
)
Linear(in_features=1280, out_features=64, bias=True)
file name: mg_catsiam_MobileNetV3_SOPClass_96_100_triplet_all_triplet_0.0001_64
>> Epoch 1/100
acc: 64.805/64.805 loss: 0.930
>> Epoch 2/100
acc: 73.656/73.656 loss: 0.585
>> Epoch 3/100
acc: 76.694/76.694 loss: 0.497
>> Epoch 4/100
acc: 78.669/78.669 loss: 0.442
>> Epoch 5/100
acc: 79.992/79.992 loss: 0.405
>> Epoch 6/100
acc: 81.062/81.062 loss: 0.371
>> Epoch 7/100
acc: 82.512/82.512 loss: 0.343
>> Epoch 8/100
acc: 83.379/83.379 loss: 0.322
>> Epoch 9/100
acc: 84.041/84.041 loss: 0.305
>> Epoch 10/100
acc: 84.880/84.880 loss: 0.288
>> Epoch 11/100
acc: 85.619/85.619 loss: 0.272
>> Epoch 12/100
acc: 86.245/86.245 loss: 0.261
>> Epoch 13/100
acc: 86.667/86.667 loss: 0.247
>> Epoch 14/100
acc: 87.315/87.315 loss: 0.235
>> Epoch 15/100
acc: 87.696/87.696 loss: 0.233
>> Epoch 16/100
acc: 88.240/88.240 loss: 0.221
>> Epoch 17/100
acc: 88.386/88.386 loss: 0.217
>> Epoch 18/100
acc: 89.003/89.003 loss: 0.205
>> Epoch 19/100
acc: 89.189/89.189 loss: 0.202
>> Epoch 20/100
acc: 89.575/89.575 loss: 0.194
>> Epoch 21/100
acc: 90.971/90.971 loss: 0.166
>> Epoch 22/100
acc: 91.004/91.004 loss: 0.166
>> Epoch 23/100
acc: 91.525/91.525 loss: 0.159
>> Epoch 24/100
acc: 91.436/91.525 loss: 0.157
>> Epoch 25/100
acc: 91.740/91.740 loss: 0.152
>> Epoch 26/100
acc: 91.871/91.871 loss: 0.152
>> Epoch 27/100
acc: 92.150/92.150 loss: 0.149
>> Epoch 28/100
acc: 92.180/92.180 loss: 0.145
>> Epoch 29/100
acc: 92.240/92.240 loss: 0.143
>> Epoch 30/100
acc: 92.240/92.240 loss: 0.144
>> Epoch 31/100
acc: 92.452/92.452 loss: 0.141
>> Epoch 32/100
acc: 92.559/92.559 loss: 0.137
>> Epoch 33/100
acc: 92.632/92.632 loss: 0.137
>> Epoch 34/100
acc: 92.682/92.682 loss: 0.136
>> Epoch 35/100
acc: 92.999/92.999 loss: 0.134
>> Epoch 36/100
acc: 92.935/92.999 loss: 0.132
>> Epoch 37/100
acc: 93.066/93.066 loss: 0.129
>> Epoch 38/100
acc: 93.140/93.140 loss: 0.127
>> Epoch 39/100
acc: 93.194/93.194 loss: 0.125
>> Epoch 40/100
acc: 93.459/93.459 loss: 0.122
>> Epoch 41/100
acc: 93.792/93.792 loss: 0.118
>> Epoch 42/100
acc: 93.930/93.930 loss: 0.115
>> Epoch 43/100
acc: 94.163/94.163 loss: 0.111
>> Epoch 44/100
acc: 94.010/94.163 loss: 0.115
>> Epoch 45/100
acc: 94.027/94.163 loss: 0.108
>> Epoch 46/100
acc: 94.304/94.304 loss: 0.109
>> Epoch 47/100
acc: 94.457/94.457 loss: 0.105
>> Epoch 48/100
acc: 94.183/94.457 loss: 0.108
>> Epoch 49/100
acc: 94.370/94.457 loss: 0.104
>> Epoch 50/100
acc: 94.433/94.457 loss: 0.102
>> Epoch 51/100
acc: 94.472/94.472 loss: 0.104
>> Epoch 52/100
acc: 94.432/94.472 loss: 0.102
>> Epoch 53/100
acc: 94.628/94.628 loss: 0.101
>> Epoch 54/100
acc: 94.640/94.640 loss: 0.101
>> Epoch 55/100
acc: 94.553/94.640 loss: 0.104
>> Epoch 56/100
acc: 94.531/94.640 loss: 0.101
>> Epoch 57/100
acc: 94.586/94.640 loss: 0.100
>> Epoch 58/100
acc: 94.667/94.667 loss: 0.099
>> Epoch 59/100
acc: 94.650/94.667 loss: 0.102
>> Epoch 60/100
acc: 94.675/94.675 loss: 0.099
>> Epoch 61/100
acc: 94.845/94.845 loss: 0.096
>> Epoch 62/100
acc: 94.907/94.907 loss: 0.097
>> Epoch 63/100
acc: 95.137/95.137 loss: 0.091
>> Epoch 64/100
acc: 95.085/95.137 loss: 0.091
>> Epoch 65/100
acc: 95.024/95.137 loss: 0.095
>> Epoch 66/100
acc: 95.132/95.137 loss: 0.093
>> Epoch 67/100
acc: 95.135/95.137 loss: 0.093
>> Epoch 68/100
acc: 95.171/95.171 loss: 0.090
>> Epoch 69/100
acc: 95.176/95.176 loss: 0.090
>> Epoch 70/100
acc: 95.374/95.374 loss: 0.090
>> Epoch 71/100
acc: 95.229/95.374 loss: 0.087
>> Epoch 72/100
acc: 95.134/95.374 loss: 0.091
>> Epoch 73/100
acc: 95.248/95.374 loss: 0.088
>> Epoch 74/100
acc: 95.276/95.374 loss: 0.088
>> Epoch 75/100
acc: 95.172/95.374 loss: 0.090
>> Epoch 76/100
acc: 95.328/95.374 loss: 0.087
>> Epoch 77/100
acc: 95.300/95.374 loss: 0.089
>> Epoch 78/100
acc: 95.172/95.374 loss: 0.091
>> Epoch 79/100
acc: 95.359/95.374 loss: 0.087
>> Epoch 80/100
acc: 95.265/95.374 loss: 0.089
>> Epoch 81/100
acc: 95.547/95.547 loss: 0.083
>> Epoch 82/100
acc: 95.481/95.547 loss: 0.085
>> Epoch 83/100
acc: 95.548/95.548 loss: 0.082
>> Epoch 84/100
acc: 95.394/95.548 loss: 0.085
>> Epoch 85/100
acc: 95.508/95.548 loss: 0.085
>> Epoch 86/100
acc: 95.481/95.548 loss: 0.086
>> Epoch 87/100
acc: 95.681/95.681 loss: 0.080
>> Epoch 88/100
acc: 95.723/95.723 loss: 0.080
>> Epoch 89/100
acc: 95.558/95.723 loss: 0.082
>> Epoch 90/100
acc: 95.589/95.723 loss: 0.080
>> Epoch 91/100
acc: 95.666/95.723 loss: 0.082
>> Epoch 92/100
acc: 95.548/95.723 loss: 0.084
>> Epoch 93/100
acc: 95.548/95.723 loss: 0.082
>> Epoch 94/100
acc: 95.668/95.723 loss: 0.082
>> Epoch 95/100
acc: 95.616/95.723 loss: 0.082
>> Epoch 96/100
acc: 95.688/95.723 loss: 0.080
>> Epoch 97/100
acc: 95.814/95.814 loss: 0.080
>> Epoch 98/100
acc: 95.703/95.814 loss: 0.082
>> Epoch 99/100
acc: 95.861/95.861 loss: 0.077
>> Epoch 100/100
acc: 95.715/95.861 loss: 0.079
mg_catsiam_MobileNetV3_SOPClass_96_100_triplet_all_triplet_0.0001_64  saved.
mg_catsiam_emb_MobileNetV3_SOPClass_96_100_triplet_all_triplet_0.0001_64  saved.
== Validation ==

Accuracy (%): 95.8606948852539	 Error rate (%): 4.139305114746094
Finished. Total elapsed time (h:m:s): 7:43:45
Currently using GPU: 0
Creating dataset: SOPClass
create labels..
Creating model: mobilenet
MobileNet(
  (base_model): MobileNetV3(
    (features): Sequential(
      (0): ConvBNActivation(
        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
      (1): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (2): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (3): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (4): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (5): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (6): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (7): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (8): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (9): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (10): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (11): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (12): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (13): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (14): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (15): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (16): ConvBNActivation(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (classifier): Sequential(
      (0): Linear(in_features=960, out_features=1280, bias=True)
      (1): Hardswish()
      (2): Dropout(p=0.2, inplace=True)
      (3): Linear(in_features=1280, out_features=12, bias=True)
    )
  )
)
Linear(in_features=1280, out_features=64, bias=True)
file name: mg_catsiam_MobileNetV3_SOPClass_96_100_triplet_easy_triplet_0.0001_64
>> Epoch 1/100
acc: 64.855/64.855 loss: 0.930
>> Epoch 2/100
acc: 73.648/73.648 loss: 0.584
>> Epoch 3/100
acc: 76.838/76.838 loss: 0.496
>> Epoch 4/100
acc: 78.739/78.739 loss: 0.442
>> Epoch 5/100
acc: 80.031/80.031 loss: 0.406
>> Epoch 6/100
acc: 81.127/81.127 loss: 0.371
>> Epoch 7/100
acc: 82.506/82.506 loss: 0.344
>> Epoch 8/100
acc: 83.389/83.389 loss: 0.322
>> Epoch 9/100
acc: 84.015/84.015 loss: 0.306
>> Epoch 10/100
acc: 84.902/84.902 loss: 0.289
>> Epoch 11/100
acc: 85.562/85.562 loss: 0.272
>> Epoch 12/100
acc: 86.229/86.229 loss: 0.261
>> Epoch 13/100
acc: 86.833/86.833 loss: 0.246
>> Epoch 14/100
acc: 87.322/87.322 loss: 0.236
>> Epoch 15/100
acc: 87.841/87.841 loss: 0.232
>> Epoch 16/100
acc: 88.192/88.192 loss: 0.221
>> Epoch 17/100
acc: 88.440/88.440 loss: 0.216
>> Epoch 18/100
acc: 88.993/88.993 loss: 0.205
>> Epoch 19/100
acc: 89.161/89.161 loss: 0.201
>> Epoch 20/100
acc: 89.616/89.616 loss: 0.194
>> Epoch 21/100
acc: 91.038/91.038 loss: 0.167
>> Epoch 22/100
acc: 90.983/91.038 loss: 0.166
>> Epoch 23/100
acc: 91.515/91.515 loss: 0.159
>> Epoch 24/100
acc: 91.505/91.515 loss: 0.157
>> Epoch 25/100
acc: 91.718/91.718 loss: 0.152
>> Epoch 26/100
acc: 91.824/91.824 loss: 0.153
>> Epoch 27/100
acc: 92.081/92.081 loss: 0.149
>> Epoch 28/100
acc: 92.245/92.245 loss: 0.144
>> Epoch 29/100
acc: 92.230/92.245 loss: 0.142
>> Epoch 30/100
acc: 92.262/92.262 loss: 0.144
>> Epoch 31/100
acc: 92.400/92.400 loss: 0.142
>> Epoch 32/100
acc: 92.586/92.586 loss: 0.137
>> Epoch 33/100
acc: 92.600/92.600 loss: 0.137
>> Epoch 34/100
acc: 92.734/92.734 loss: 0.136
>> Epoch 35/100
acc: 92.944/92.944 loss: 0.133
>> Epoch 36/100
acc: 92.909/92.944 loss: 0.131
>> Epoch 37/100
acc: 93.082/93.082 loss: 0.128
>> Epoch 38/100
acc: 93.147/93.147 loss: 0.128
>> Epoch 39/100
acc: 93.224/93.224 loss: 0.125
>> Epoch 40/100
acc: 93.466/93.466 loss: 0.121
>> Epoch 41/100
acc: 93.819/93.819 loss: 0.118
>> Epoch 42/100
acc: 93.898/93.898 loss: 0.115
>> Epoch 43/100
acc: 94.170/94.170 loss: 0.111
>> Epoch 44/100
acc: 94.025/94.170 loss: 0.114
>> Epoch 45/100
acc: 94.034/94.170 loss: 0.108
>> Epoch 46/100
acc: 94.245/94.245 loss: 0.109
>> Epoch 47/100
acc: 94.395/94.395 loss: 0.105
>> Epoch 48/100
acc: 94.207/94.395 loss: 0.107
>> Epoch 49/100
acc: 94.292/94.395 loss: 0.104
>> Epoch 50/100
acc: 94.385/94.395 loss: 0.102
>> Epoch 51/100
acc: 94.487/94.487 loss: 0.103
>> Epoch 52/100
acc: 94.430/94.487 loss: 0.102
>> Epoch 53/100
acc: 94.665/94.665 loss: 0.101
>> Epoch 54/100
acc: 94.578/94.665 loss: 0.101
>> Epoch 55/100
acc: 94.608/94.665 loss: 0.104
>> Epoch 56/100
acc: 94.490/94.665 loss: 0.101
>> Epoch 57/100
acc: 94.623/94.665 loss: 0.100
>> Epoch 58/100
acc: 94.707/94.707 loss: 0.098
>> Epoch 59/100
acc: 94.633/94.707 loss: 0.102
>> Epoch 60/100
acc: 94.689/94.707 loss: 0.099
>> Epoch 61/100
acc: 94.850/94.850 loss: 0.096
>> Epoch 62/100
acc: 94.976/94.976 loss: 0.097
>> Epoch 63/100
acc: 95.120/95.120 loss: 0.091
>> Epoch 64/100
acc: 95.117/95.120 loss: 0.091
>> Epoch 65/100
acc: 94.999/95.120 loss: 0.095
>> Epoch 66/100
acc: 95.155/95.155 loss: 0.093
>> Epoch 67/100
acc: 95.107/95.155 loss: 0.093
>> Epoch 68/100
acc: 95.238/95.238 loss: 0.090
>> Epoch 69/100
acc: 95.201/95.238 loss: 0.090
>> Epoch 70/100
acc: 95.298/95.298 loss: 0.090
>> Epoch 71/100
acc: 95.229/95.298 loss: 0.087
>> Epoch 72/100
acc: 95.179/95.298 loss: 0.091
>> Epoch 73/100
acc: 95.236/95.298 loss: 0.089
>> Epoch 74/100
acc: 95.271/95.298 loss: 0.088
>> Epoch 75/100
acc: 95.207/95.298 loss: 0.090
>> Epoch 76/100
acc: 95.317/95.317 loss: 0.087
>> Epoch 77/100
acc: 95.263/95.317 loss: 0.089
>> Epoch 78/100
acc: 95.181/95.317 loss: 0.090
>> Epoch 79/100
acc: 95.394/95.394 loss: 0.087
>> Epoch 80/100
acc: 95.278/95.394 loss: 0.090
>> Epoch 81/100
acc: 95.530/95.530 loss: 0.083
>> Epoch 82/100
acc: 95.528/95.530 loss: 0.084
>> Epoch 83/100
acc: 95.543/95.543 loss: 0.082
>> Epoch 84/100
acc: 95.451/95.543 loss: 0.085
>> Epoch 85/100
acc: 95.545/95.545 loss: 0.084
>> Epoch 86/100
acc: 95.427/95.545 loss: 0.086
>> Epoch 87/100
acc: 95.710/95.710 loss: 0.081
>> Epoch 88/100
acc: 95.783/95.783 loss: 0.080
>> Epoch 89/100
acc: 95.610/95.783 loss: 0.082
>> Epoch 90/100
acc: 95.610/95.783 loss: 0.080
>> Epoch 91/100
acc: 95.624/95.783 loss: 0.082
>> Epoch 92/100
acc: 95.621/95.783 loss: 0.084
>> Epoch 93/100
acc: 95.597/95.783 loss: 0.081
>> Epoch 94/100
acc: 95.715/95.783 loss: 0.082
>> Epoch 95/100
acc: 95.599/95.783 loss: 0.082
>> Epoch 96/100
acc: 95.731/95.783 loss: 0.080
>> Epoch 97/100
acc: 95.802/95.802 loss: 0.079
>> Epoch 98/100
acc: 95.664/95.802 loss: 0.081
>> Epoch 99/100
acc: 95.825/95.825 loss: 0.077
>> Epoch 100/100
acc: 95.703/95.825 loss: 0.080
mg_catsiam_MobileNetV3_SOPClass_96_100_triplet_easy_triplet_0.0001_64  saved.
mg_catsiam_emb_MobileNetV3_SOPClass_96_100_triplet_easy_triplet_0.0001_64  saved.
== Validation ==

Accuracy (%): 95.82543182373047	 Error rate (%): 4.174568176269531
Finished. Total elapsed time (h:m:s): 7:42:35
Currently using GPU: 0
Creating dataset: SOPClass
create labels..
Creating model: mobilenet
MobileNet(
  (base_model): MobileNetV3(
    (features): Sequential(
      (0): ConvBNActivation(
        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
      (1): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (2): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (3): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (4): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (5): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (6): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (7): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (8): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (9): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (10): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (11): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (12): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (13): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (14): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (15): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (16): ConvBNActivation(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (classifier): Sequential(
      (0): Linear(in_features=960, out_features=1280, bias=True)
      (1): Hardswish()
      (2): Dropout(p=0.2, inplace=True)
      (3): Linear(in_features=1280, out_features=12, bias=True)
    )
  )
)
Linear(in_features=1280, out_features=64, bias=True)
file name: mg_catsiam_MobileNetV3_SOPClass_96_100_triplet_semi_triplet_0.0001_64
>> Epoch 1/100
acc: 64.849/64.849 loss: 0.929
>> Epoch 2/100
acc: 73.623/73.623 loss: 0.585
>> Epoch 3/100
acc: 76.756/76.756 loss: 0.496
>> Epoch 4/100
acc: 78.665/78.665 loss: 0.442
>> Epoch 5/100
acc: 80.041/80.041 loss: 0.406
>> Epoch 6/100
acc: 81.174/81.174 loss: 0.371
>> Epoch 7/100
acc: 82.509/82.509 loss: 0.343
>> Epoch 8/100
acc: 83.275/83.275 loss: 0.322
>> Epoch 9/100
acc: 84.072/84.072 loss: 0.305
>> Epoch 10/100
acc: 84.842/84.842 loss: 0.289
>> Epoch 11/100
acc: 85.517/85.517 loss: 0.271
>> Epoch 12/100
acc: 86.219/86.219 loss: 0.261
>> Epoch 13/100
acc: 86.709/86.709 loss: 0.247
>> Epoch 14/100
acc: 87.238/87.238 loss: 0.236
>> Epoch 15/100
acc: 87.713/87.713 loss: 0.233
>> Epoch 16/100
acc: 88.262/88.262 loss: 0.221
>> Epoch 17/100
acc: 88.329/88.329 loss: 0.218
>> Epoch 18/100
acc: 88.999/88.999 loss: 0.205
>> Epoch 19/100
acc: 89.152/89.152 loss: 0.202
>> Epoch 20/100
acc: 89.560/89.560 loss: 0.196
>> Epoch 21/100
acc: 90.917/90.917 loss: 0.166
>> Epoch 22/100
acc: 91.043/91.043 loss: 0.166
>> Epoch 23/100
acc: 91.520/91.520 loss: 0.158
>> Epoch 24/100
acc: 91.533/91.533 loss: 0.156
>> Epoch 25/100
acc: 91.748/91.748 loss: 0.152
>> Epoch 26/100
acc: 91.780/91.780 loss: 0.153
>> Epoch 27/100
acc: 92.025/92.025 loss: 0.149
>> Epoch 28/100
acc: 92.297/92.297 loss: 0.144
>> Epoch 29/100
acc: 92.284/92.297 loss: 0.143
>> Epoch 30/100
acc: 92.279/92.297 loss: 0.143
>> Epoch 31/100
acc: 92.462/92.462 loss: 0.142
>> Epoch 32/100
acc: 92.611/92.611 loss: 0.137
>> Epoch 33/100
acc: 92.620/92.620 loss: 0.137
>> Epoch 34/100
acc: 92.652/92.652 loss: 0.136
>> Epoch 35/100
acc: 92.974/92.974 loss: 0.134
>> Epoch 36/100
acc: 92.915/92.974 loss: 0.132
>> Epoch 37/100
acc: 93.164/93.164 loss: 0.128
>> Epoch 38/100
acc: 93.140/93.164 loss: 0.127
>> Epoch 39/100
acc: 93.145/93.164 loss: 0.125
>> Epoch 40/100
acc: 93.434/93.434 loss: 0.121
>> Epoch 41/100
acc: 93.820/93.820 loss: 0.117
>> Epoch 42/100
acc: 93.893/93.893 loss: 0.115
>> Epoch 43/100
acc: 94.123/94.123 loss: 0.111
>> Epoch 44/100
acc: 94.124/94.124 loss: 0.115
>> Epoch 45/100
acc: 94.146/94.146 loss: 0.108
>> Epoch 46/100
acc: 94.247/94.247 loss: 0.109
>> Epoch 47/100
acc: 94.393/94.393 loss: 0.104
>> Epoch 48/100
acc: 94.267/94.393 loss: 0.107
>> Epoch 49/100
acc: 94.348/94.393 loss: 0.104
>> Epoch 50/100
acc: 94.401/94.401 loss: 0.102
>> Epoch 51/100
acc: 94.558/94.558 loss: 0.103
>> Epoch 52/100
acc: 94.485/94.558 loss: 0.102
>> Epoch 53/100
acc: 94.584/94.584 loss: 0.101
>> Epoch 54/100
acc: 94.584/94.584 loss: 0.101
>> Epoch 55/100
acc: 94.581/94.584 loss: 0.104
>> Epoch 56/100
acc: 94.564/94.584 loss: 0.101
>> Epoch 57/100
acc: 94.647/94.647 loss: 0.100
>> Epoch 58/100
acc: 94.685/94.685 loss: 0.098
>> Epoch 59/100
acc: 94.633/94.685 loss: 0.102
>> Epoch 60/100
acc: 94.652/94.685 loss: 0.099
>> Epoch 61/100
acc: 94.816/94.816 loss: 0.096
>> Epoch 62/100
acc: 94.917/94.917 loss: 0.096
>> Epoch 63/100
acc: 95.137/95.137 loss: 0.091
>> Epoch 64/100
acc: 95.113/95.137 loss: 0.091
>> Epoch 65/100
acc: 95.055/95.137 loss: 0.095
>> Epoch 66/100
acc: 95.140/95.140 loss: 0.093
>> Epoch 67/100
acc: 95.066/95.140 loss: 0.093
>> Epoch 68/100
acc: 95.239/95.239 loss: 0.090
>> Epoch 69/100
acc: 95.207/95.239 loss: 0.090
>> Epoch 70/100
acc: 95.327/95.327 loss: 0.089
>> Epoch 71/100
acc: 95.296/95.327 loss: 0.087
>> Epoch 72/100
acc: 95.157/95.327 loss: 0.091
>> Epoch 73/100
acc: 95.244/95.327 loss: 0.088
>> Epoch 74/100
acc: 95.275/95.327 loss: 0.089
>> Epoch 75/100
acc: 95.165/95.327 loss: 0.090
>> Epoch 76/100
acc: 95.370/95.370 loss: 0.087
>> Epoch 77/100
acc: 95.318/95.370 loss: 0.089
>> Epoch 78/100
acc: 95.087/95.370 loss: 0.090
>> Epoch 79/100
acc: 95.399/95.399 loss: 0.087
>> Epoch 80/100
acc: 95.302/95.399 loss: 0.089
>> Epoch 81/100
acc: 95.500/95.500 loss: 0.083
>> Epoch 82/100
acc: 95.511/95.511 loss: 0.085
>> Epoch 83/100
acc: 95.572/95.572 loss: 0.083
>> Epoch 84/100
acc: 95.446/95.572 loss: 0.084
>> Epoch 85/100
acc: 95.565/95.572 loss: 0.084
>> Epoch 86/100
acc: 95.439/95.572 loss: 0.086
>> Epoch 87/100
acc: 95.708/95.708 loss: 0.080
>> Epoch 88/100
acc: 95.706/95.708 loss: 0.080
>> Epoch 89/100
acc: 95.649/95.708 loss: 0.082
>> Epoch 90/100
acc: 95.639/95.708 loss: 0.080
>> Epoch 91/100
acc: 95.636/95.708 loss: 0.082
>> Epoch 92/100
acc: 95.649/95.708 loss: 0.083
>> Epoch 93/100
acc: 95.585/95.708 loss: 0.081
>> Epoch 94/100
acc: 95.666/95.708 loss: 0.082
>> Epoch 95/100
acc: 95.626/95.708 loss: 0.082
>> Epoch 96/100
acc: 95.733/95.733 loss: 0.080
>> Epoch 97/100
acc: 95.758/95.758 loss: 0.080
>> Epoch 98/100
acc: 95.713/95.758 loss: 0.082
>> Epoch 99/100
acc: 95.799/95.799 loss: 0.077
>> Epoch 100/100
acc: 95.679/95.799 loss: 0.079
mg_catsiam_MobileNetV3_SOPClass_96_100_triplet_semi_triplet_0.0001_64  saved.
mg_catsiam_emb_MobileNetV3_SOPClass_96_100_triplet_semi_triplet_0.0001_64  saved.
== Validation ==

Accuracy (%): 95.7985610961914	 Error rate (%): 4.201438903808594
Finished. Total elapsed time (h:m:s): 7:43:06
Currently using GPU: 0
Creating dataset: SOPClass
create labels..
Creating model: mobilenet
MobileNet(
  (base_model): MobileNetV3(
    (features): Sequential(
      (0): ConvBNActivation(
        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
      (1): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (2): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (3): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (4): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (5): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (6): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (7): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (8): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (9): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (10): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (11): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (12): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (13): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (14): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (15): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (16): ConvBNActivation(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (classifier): Sequential(
      (0): Linear(in_features=960, out_features=1280, bias=True)
      (1): Hardswish()
      (2): Dropout(p=0.2, inplace=True)
      (3): Linear(in_features=1280, out_features=12, bias=True)
    )
  )
)
Linear(in_features=1280, out_features=64, bias=True)
file name: mg_catsiam_MobileNetV3_SOPClass_96_100_triplet_hard_triplet_0.0001_64
>> Epoch 1/100
acc: 64.830/64.830 loss: 0.930
>> Epoch 2/100
acc: 73.639/73.639 loss: 0.585
>> Epoch 3/100
acc: 76.721/76.721 loss: 0.496
>> Epoch 4/100
acc: 78.699/78.699 loss: 0.442
>> Epoch 5/100
acc: 80.014/80.014 loss: 0.405
>> Epoch 6/100
acc: 81.129/81.129 loss: 0.371
>> Epoch 7/100
acc: 82.484/82.484 loss: 0.344
>> Epoch 8/100
acc: 83.376/83.376 loss: 0.322
>> Epoch 9/100
acc: 84.059/84.059 loss: 0.306
>> Epoch 10/100
acc: 84.840/84.840 loss: 0.289
>> Epoch 11/100
acc: 85.555/85.555 loss: 0.271
>> Epoch 12/100
acc: 86.207/86.207 loss: 0.261
>> Epoch 13/100
acc: 86.679/86.679 loss: 0.247
>> Epoch 14/100
acc: 87.219/87.219 loss: 0.237
>> Epoch 15/100
acc: 87.777/87.777 loss: 0.232
>> Epoch 16/100
acc: 88.220/88.220 loss: 0.220
>> Epoch 17/100
acc: 88.360/88.360 loss: 0.217
>> Epoch 18/100
acc: 89.016/89.016 loss: 0.205
>> Epoch 19/100
acc: 89.265/89.265 loss: 0.201
>> Epoch 20/100
acc: 89.616/89.616 loss: 0.194
>> Epoch 21/100
acc: 90.991/90.991 loss: 0.166
>> Epoch 22/100
acc: 91.055/91.055 loss: 0.166
>> Epoch 23/100
acc: 91.454/91.454 loss: 0.158
>> Epoch 24/100
acc: 91.520/91.520 loss: 0.157
>> Epoch 25/100
acc: 91.795/91.795 loss: 0.152
>> Epoch 26/100
acc: 91.851/91.851 loss: 0.152
>> Epoch 27/100
acc: 92.082/92.082 loss: 0.149
>> Epoch 28/100
acc: 92.316/92.316 loss: 0.145
>> Epoch 29/100
acc: 92.195/92.316 loss: 0.142
>> Epoch 30/100
acc: 92.213/92.316 loss: 0.143
>> Epoch 31/100
acc: 92.432/92.432 loss: 0.142
>> Epoch 32/100
acc: 92.571/92.571 loss: 0.137
>> Epoch 33/100
acc: 92.637/92.637 loss: 0.137
>> Epoch 34/100
acc: 92.739/92.739 loss: 0.136
>> Epoch 35/100
acc: 93.009/93.009 loss: 0.134
>> Epoch 36/100
acc: 93.038/93.038 loss: 0.131
>> Epoch 37/100
acc: 93.090/93.090 loss: 0.128
>> Epoch 38/100
acc: 93.166/93.166 loss: 0.126
>> Epoch 39/100
acc: 93.265/93.265 loss: 0.125
>> Epoch 40/100
acc: 93.493/93.493 loss: 0.121
>> Epoch 41/100
acc: 93.862/93.862 loss: 0.118
>> Epoch 42/100
acc: 93.940/93.940 loss: 0.115
>> Epoch 43/100
acc: 94.218/94.218 loss: 0.111
>> Epoch 44/100
acc: 94.047/94.218 loss: 0.114
>> Epoch 45/100
acc: 94.091/94.218 loss: 0.108
>> Epoch 46/100
acc: 94.269/94.269 loss: 0.109
>> Epoch 47/100
acc: 94.417/94.417 loss: 0.105
>> Epoch 48/100
acc: 94.207/94.417 loss: 0.107
>> Epoch 49/100
acc: 94.316/94.417 loss: 0.104
>> Epoch 50/100
acc: 94.390/94.417 loss: 0.103
>> Epoch 51/100
acc: 94.465/94.465 loss: 0.104
>> Epoch 52/100
acc: 94.413/94.465 loss: 0.103
>> Epoch 53/100
acc: 94.610/94.610 loss: 0.101
>> Epoch 54/100
acc: 94.618/94.618 loss: 0.101
>> Epoch 55/100
acc: 94.568/94.618 loss: 0.104
>> Epoch 56/100
acc: 94.569/94.618 loss: 0.100
>> Epoch 57/100
acc: 94.630/94.630 loss: 0.100
>> Epoch 58/100
acc: 94.722/94.722 loss: 0.098
>> Epoch 59/100
acc: 94.653/94.722 loss: 0.102
>> Epoch 60/100
acc: 94.635/94.722 loss: 0.100
>> Epoch 61/100
acc: 94.840/94.840 loss: 0.096
>> Epoch 62/100
acc: 94.986/94.986 loss: 0.096
>> Epoch 63/100
acc: 95.191/95.191 loss: 0.091
>> Epoch 64/100
acc: 95.073/95.191 loss: 0.091
>> Epoch 65/100
acc: 95.016/95.191 loss: 0.094
>> Epoch 66/100
acc: 95.122/95.191 loss: 0.093
>> Epoch 67/100
acc: 95.073/95.191 loss: 0.093
>> Epoch 68/100
acc: 95.179/95.191 loss: 0.090
>> Epoch 69/100
acc: 95.150/95.191 loss: 0.090
>> Epoch 70/100
acc: 95.354/95.354 loss: 0.089
>> Epoch 71/100
acc: 95.263/95.354 loss: 0.088
>> Epoch 72/100
acc: 95.120/95.354 loss: 0.091
>> Epoch 73/100
acc: 95.323/95.354 loss: 0.088
>> Epoch 74/100
acc: 95.256/95.354 loss: 0.088
>> Epoch 75/100
acc: 95.144/95.354 loss: 0.091
>> Epoch 76/100
acc: 95.302/95.354 loss: 0.087
>> Epoch 77/100
acc: 95.374/95.374 loss: 0.088
>> Epoch 78/100
acc: 95.192/95.374 loss: 0.090
>> Epoch 79/100
acc: 95.369/95.374 loss: 0.087
>> Epoch 80/100
acc: 95.280/95.374 loss: 0.089
>> Epoch 81/100
acc: 95.553/95.553 loss: 0.082
>> Epoch 82/100
acc: 95.540/95.553 loss: 0.084
>> Epoch 83/100
acc: 95.550/95.553 loss: 0.082
>> Epoch 84/100
acc: 95.491/95.553 loss: 0.084
>> Epoch 85/100
acc: 95.520/95.553 loss: 0.084
>> Epoch 86/100
acc: 95.535/95.553 loss: 0.086
>> Epoch 87/100
acc: 95.658/95.658 loss: 0.080
>> Epoch 88/100
acc: 95.706/95.706 loss: 0.080
>> Epoch 89/100
acc: 95.656/95.706 loss: 0.082
>> Epoch 90/100
acc: 95.696/95.706 loss: 0.079
>> Epoch 91/100
acc: 95.646/95.706 loss: 0.083
>> Epoch 92/100
acc: 95.584/95.706 loss: 0.084
>> Epoch 93/100
acc: 95.610/95.706 loss: 0.081
>> Epoch 94/100
acc: 95.678/95.706 loss: 0.081
>> Epoch 95/100
acc: 95.663/95.706 loss: 0.082
>> Epoch 96/100
acc: 95.718/95.718 loss: 0.080
>> Epoch 97/100
acc: 95.825/95.825 loss: 0.079
>> Epoch 98/100
acc: 95.726/95.825 loss: 0.081
>> Epoch 99/100
acc: 95.866/95.866 loss: 0.077
>> Epoch 100/100
acc: 95.683/95.866 loss: 0.080
mg_catsiam_MobileNetV3_SOPClass_96_100_triplet_hard_triplet_0.0001_64  saved.
mg_catsiam_emb_MobileNetV3_SOPClass_96_100_triplet_hard_triplet_0.0001_64  saved.
== Validation ==

Accuracy (%): 95.86573028564453	 Error rate (%): 4.134269714355469
Finished. Total elapsed time (h:m:s): 7:43:17
