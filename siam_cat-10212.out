Currently using GPU: 0
Creating dataset: SOPClass
create labels..
Creating model: mobilenet
MobileNet(
  (base_model): MobileNetV3(
    (features): Sequential(
      (0): ConvBNActivation(
        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
      (1): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (2): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (3): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (4): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (5): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (6): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (7): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (8): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (9): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (10): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (11): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (12): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (13): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (14): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (15): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (16): ConvBNActivation(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (classifier): Sequential(
      (0): Linear(in_features=960, out_features=1280, bias=True)
      (1): Hardswish()
      (2): Dropout(p=0.2, inplace=True)
      (3): Linear(in_features=1280, out_features=12, bias=True)
    )
  )
)
Linear(in_features=1280, out_features=256, bias=True)
file name: mg_catsiam_MobileNetV3_SOPClass_96_100_pairmargin_pair_0.0001_256
>> Epoch 1/100
acc: 64.565/64.565 loss: 0.933
>> Epoch 2/100
acc: 73.707/73.707 loss: 0.591
>> Epoch 3/100
acc: 76.440/76.440 loss: 0.500
>> Epoch 4/100
acc: 78.323/78.323 loss: 0.452
>> Epoch 5/100
acc: 79.881/79.881 loss: 0.400
>> Epoch 6/100
acc: 81.522/81.522 loss: 0.368
>> Epoch 7/100
acc: 82.398/82.398 loss: 0.338
>> Epoch 8/100
acc: 83.381/83.381 loss: 0.323
>> Epoch 9/100
acc: 84.232/84.232 loss: 0.305
>> Epoch 10/100
acc: 84.842/84.842 loss: 0.287
>> Epoch 11/100
acc: 85.607/85.607 loss: 0.270
>> Epoch 12/100
acc: 86.178/86.178 loss: 0.256
>> Epoch 13/100
acc: 87.008/87.008 loss: 0.246
>> Epoch 14/100
acc: 87.276/87.276 loss: 0.240
>> Epoch 15/100
acc: 87.973/87.973 loss: 0.225
>> Epoch 16/100
acc: 88.219/88.219 loss: 0.219
>> Epoch 17/100
acc: 88.586/88.586 loss: 0.213
>> Epoch 18/100
acc: 88.899/88.899 loss: 0.209
>> Epoch 19/100
acc: 89.439/89.439 loss: 0.199
>> Epoch 20/100
acc: 89.636/89.636 loss: 0.196
>> Epoch 21/100
acc: 90.417/90.417 loss: 0.178
>> Epoch 22/100
acc: 91.092/91.092 loss: 0.166
>> Epoch 23/100
acc: 91.278/91.278 loss: 0.161
>> Epoch 24/100
acc: 91.713/91.713 loss: 0.155
>> Epoch 25/100
acc: 91.557/91.713 loss: 0.155
>> Epoch 26/100
acc: 92.040/92.040 loss: 0.147
>> Epoch 27/100
acc: 92.094/92.094 loss: 0.147
>> Epoch 28/100
acc: 92.237/92.237 loss: 0.144
>> Epoch 29/100
acc: 92.104/92.237 loss: 0.147
>> Epoch 30/100
acc: 92.647/92.647 loss: 0.139
>> Epoch 31/100
acc: 92.529/92.647 loss: 0.139
>> Epoch 32/100
acc: 92.509/92.647 loss: 0.144
>> Epoch 33/100
acc: 92.756/92.756 loss: 0.133
>> Epoch 34/100
acc: 92.974/92.974 loss: 0.131
>> Epoch 35/100
acc: 92.917/92.974 loss: 0.132
>> Epoch 36/100
acc: 92.922/92.974 loss: 0.131
>> Epoch 37/100
acc: 93.035/93.035 loss: 0.132
>> Epoch 38/100
acc: 93.214/93.214 loss: 0.128
>> Epoch 39/100
acc: 93.124/93.214 loss: 0.130
>> Epoch 40/100
acc: 93.271/93.271 loss: 0.125
>> Epoch 41/100
acc: 93.972/93.972 loss: 0.112
>> Epoch 42/100
acc: 93.983/93.983 loss: 0.111
>> Epoch 43/100
acc: 94.133/94.133 loss: 0.112
>> Epoch 44/100
acc: 93.973/94.133 loss: 0.112
>> Epoch 45/100
acc: 94.348/94.348 loss: 0.108
>> Epoch 46/100
acc: 94.146/94.348 loss: 0.110
>> Epoch 47/100
acc: 94.398/94.398 loss: 0.109
>> Epoch 48/100
acc: 94.349/94.398 loss: 0.105
>> Epoch 49/100
acc: 94.259/94.398 loss: 0.110
>> Epoch 50/100
acc: 94.366/94.398 loss: 0.105
>> Epoch 51/100
acc: 94.485/94.485 loss: 0.105
>> Epoch 52/100
acc: 94.526/94.526 loss: 0.103
>> Epoch 53/100
acc: 94.657/94.657 loss: 0.099
>> Epoch 54/100
acc: 94.620/94.657 loss: 0.101
>> Epoch 55/100
acc: 94.506/94.657 loss: 0.102
>> Epoch 56/100
acc: 94.621/94.657 loss: 0.100
>> Epoch 57/100
acc: 94.576/94.657 loss: 0.102
>> Epoch 58/100
acc: 94.578/94.657 loss: 0.099
>> Epoch 59/100
acc: 94.754/94.754 loss: 0.101
>> Epoch 60/100
acc: 94.862/94.862 loss: 0.098
>> Epoch 61/100
acc: 95.029/95.029 loss: 0.094
>> Epoch 62/100
acc: 95.045/95.045 loss: 0.096
>> Epoch 63/100
acc: 95.189/95.189 loss: 0.091
>> Epoch 64/100
acc: 95.097/95.189 loss: 0.091
>> Epoch 65/100
acc: 95.085/95.189 loss: 0.092
>> Epoch 66/100
acc: 95.197/95.197 loss: 0.091
>> Epoch 67/100
acc: 95.233/95.233 loss: 0.092
>> Epoch 68/100
acc: 95.159/95.233 loss: 0.090
>> Epoch 69/100
acc: 95.186/95.233 loss: 0.088
>> Epoch 70/100
acc: 95.328/95.328 loss: 0.088
>> Epoch 71/100
acc: 95.176/95.328 loss: 0.090
>> Epoch 72/100
acc: 95.233/95.328 loss: 0.091
>> Epoch 73/100
acc: 95.223/95.328 loss: 0.091
>> Epoch 74/100
acc: 95.307/95.328 loss: 0.087
>> Epoch 75/100
acc: 95.228/95.328 loss: 0.086
>> Epoch 76/100
acc: 95.391/95.391 loss: 0.087
>> Epoch 77/100
acc: 95.290/95.391 loss: 0.089
>> Epoch 78/100
acc: 95.238/95.391 loss: 0.088
>> Epoch 79/100
acc: 95.369/95.391 loss: 0.083
>> Epoch 80/100
acc: 95.360/95.391 loss: 0.086
>> Epoch 81/100
acc: 95.553/95.553 loss: 0.083
>> Epoch 82/100
acc: 95.560/95.560 loss: 0.087
>> Epoch 83/100
acc: 95.532/95.560 loss: 0.084
>> Epoch 84/100
acc: 95.600/95.600 loss: 0.081
>> Epoch 85/100
acc: 95.659/95.659 loss: 0.081
>> Epoch 86/100
acc: 95.589/95.659 loss: 0.083
>> Epoch 87/100
acc: 95.431/95.659 loss: 0.085
>> Epoch 88/100
acc: 95.500/95.659 loss: 0.086
>> Epoch 89/100
acc: 95.488/95.659 loss: 0.084
>> Epoch 90/100
acc: 95.574/95.659 loss: 0.083
>> Epoch 91/100
acc: 95.589/95.659 loss: 0.082
>> Epoch 92/100
acc: 95.641/95.659 loss: 0.083
>> Epoch 93/100
acc: 95.740/95.740 loss: 0.081
>> Epoch 94/100
acc: 95.624/95.740 loss: 0.080
>> Epoch 95/100
acc: 95.602/95.740 loss: 0.083
>> Epoch 96/100
acc: 95.688/95.740 loss: 0.080
>> Epoch 97/100
acc: 95.758/95.758 loss: 0.078
>> Epoch 98/100
acc: 95.605/95.758 loss: 0.080
>> Epoch 99/100
acc: 95.817/95.817 loss: 0.080
>> Epoch 100/100
acc: 95.493/95.817 loss: 0.083
mg_catsiam_MobileNetV3_SOPClass_96_100_pairmargin_pair_0.0001_256  saved.
mg_catsiam_emb_MobileNetV3_SOPClass_96_100_pairmargin_pair_0.0001_256  saved.
== Validation ==

Accuracy (%): 95.81703186035156	 Error rate (%): 4.1829681396484375
Finished. Total elapsed time (h:m:s): 7:44:04
Currently using GPU: 0
Creating dataset: SOPClass
create labels..
Creating model: mobilenet
MobileNet(
  (base_model): MobileNetV3(
    (features): Sequential(
      (0): ConvBNActivation(
        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
      (1): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (2): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (3): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (4): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (5): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (6): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (7): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (8): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (9): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (10): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (11): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (12): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (13): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (14): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (15): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (16): ConvBNActivation(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (classifier): Sequential(
      (0): Linear(in_features=960, out_features=1280, bias=True)
      (1): Hardswish()
      (2): Dropout(p=0.2, inplace=True)
      (3): Linear(in_features=1280, out_features=12, bias=True)
    )
  )
)
Linear(in_features=1280, out_features=256, bias=True)
file name: mg_catsiam_MobileNetV3_SOPClass_96_100_triplet_all_triplet_0.0001_256
>> Epoch 1/100
acc: 64.602/64.602 loss: 0.933
>> Epoch 2/100
acc: 73.717/73.717 loss: 0.591
>> Epoch 3/100
acc: 76.511/76.511 loss: 0.500
>> Epoch 4/100
acc: 78.351/78.351 loss: 0.452
>> Epoch 5/100
acc: 79.926/79.926 loss: 0.400
>> Epoch 6/100
acc: 81.555/81.555 loss: 0.368
>> Epoch 7/100
acc: 82.412/82.412 loss: 0.338
>> Epoch 8/100
acc: 83.349/83.349 loss: 0.324
>> Epoch 9/100
acc: 84.240/84.240 loss: 0.305
>> Epoch 10/100
acc: 84.880/84.880 loss: 0.286
>> Epoch 11/100
acc: 85.550/85.550 loss: 0.271
>> Epoch 12/100
acc: 86.269/86.269 loss: 0.256
>> Epoch 13/100
acc: 86.915/86.915 loss: 0.245
>> Epoch 14/100
acc: 87.355/87.355 loss: 0.241
>> Epoch 15/100
acc: 87.857/87.857 loss: 0.225
>> Epoch 16/100
acc: 88.247/88.247 loss: 0.219
>> Epoch 17/100
acc: 88.551/88.551 loss: 0.214
>> Epoch 18/100
acc: 88.951/88.951 loss: 0.208
>> Epoch 19/100
acc: 89.349/89.349 loss: 0.200
>> Epoch 20/100
acc: 89.698/89.698 loss: 0.195
>> Epoch 21/100
acc: 90.511/90.511 loss: 0.178
>> Epoch 22/100
acc: 91.080/91.080 loss: 0.166
>> Epoch 23/100
acc: 91.196/91.196 loss: 0.162
>> Epoch 24/100
acc: 91.706/91.706 loss: 0.155
>> Epoch 25/100
acc: 91.616/91.706 loss: 0.154
>> Epoch 26/100
acc: 92.071/92.071 loss: 0.147
>> Epoch 27/100
acc: 92.072/92.072 loss: 0.147
>> Epoch 28/100
acc: 92.240/92.240 loss: 0.144
>> Epoch 29/100
acc: 92.133/92.240 loss: 0.147
>> Epoch 30/100
acc: 92.573/92.573 loss: 0.139
>> Epoch 31/100
acc: 92.502/92.573 loss: 0.139
>> Epoch 32/100
acc: 92.477/92.573 loss: 0.144
>> Epoch 33/100
acc: 92.799/92.799 loss: 0.134
>> Epoch 34/100
acc: 92.935/92.935 loss: 0.131
>> Epoch 35/100
acc: 92.890/92.935 loss: 0.131
>> Epoch 36/100
acc: 92.961/92.961 loss: 0.131
>> Epoch 37/100
acc: 93.043/93.043 loss: 0.132
>> Epoch 38/100
acc: 93.164/93.164 loss: 0.129
>> Epoch 39/100
acc: 93.172/93.172 loss: 0.130
>> Epoch 40/100
acc: 93.216/93.216 loss: 0.125
>> Epoch 41/100
acc: 93.958/93.958 loss: 0.112
>> Epoch 42/100
acc: 94.014/94.014 loss: 0.111
>> Epoch 43/100
acc: 94.134/94.134 loss: 0.112
>> Epoch 44/100
acc: 93.931/94.134 loss: 0.112
>> Epoch 45/100
acc: 94.302/94.302 loss: 0.108
>> Epoch 46/100
acc: 94.124/94.302 loss: 0.110
>> Epoch 47/100
acc: 94.321/94.321 loss: 0.108
>> Epoch 48/100
acc: 94.338/94.338 loss: 0.105
>> Epoch 49/100
acc: 94.254/94.338 loss: 0.110
>> Epoch 50/100
acc: 94.422/94.422 loss: 0.104
>> Epoch 51/100
acc: 94.482/94.482 loss: 0.107
>> Epoch 52/100
acc: 94.462/94.482 loss: 0.103
>> Epoch 53/100
acc: 94.608/94.608 loss: 0.099
>> Epoch 54/100
acc: 94.615/94.615 loss: 0.100
>> Epoch 55/100
acc: 94.502/94.615 loss: 0.103
>> Epoch 56/100
acc: 94.640/94.640 loss: 0.100
>> Epoch 57/100
acc: 94.601/94.640 loss: 0.102
>> Epoch 58/100
acc: 94.532/94.640 loss: 0.099
>> Epoch 59/100
acc: 94.751/94.751 loss: 0.101
>> Epoch 60/100
acc: 94.757/94.757 loss: 0.098
>> Epoch 61/100
acc: 94.996/94.996 loss: 0.094
>> Epoch 62/100
acc: 95.063/95.063 loss: 0.096
>> Epoch 63/100
acc: 95.192/95.192 loss: 0.091
>> Epoch 64/100
acc: 95.078/95.192 loss: 0.092
>> Epoch 65/100
acc: 95.120/95.192 loss: 0.092
>> Epoch 66/100
acc: 95.231/95.231 loss: 0.091
>> Epoch 67/100
acc: 95.275/95.275 loss: 0.091
>> Epoch 68/100
acc: 95.140/95.275 loss: 0.091
>> Epoch 69/100
acc: 95.189/95.275 loss: 0.088
>> Epoch 70/100
acc: 95.397/95.397 loss: 0.088
>> Epoch 71/100
acc: 95.231/95.397 loss: 0.090
>> Epoch 72/100
acc: 95.228/95.397 loss: 0.091
>> Epoch 73/100
acc: 95.285/95.397 loss: 0.090
>> Epoch 74/100
acc: 95.191/95.397 loss: 0.088
>> Epoch 75/100
acc: 95.249/95.397 loss: 0.087
>> Epoch 76/100
acc: 95.394/95.397 loss: 0.087
>> Epoch 77/100
acc: 95.372/95.397 loss: 0.089
>> Epoch 78/100
acc: 95.258/95.397 loss: 0.088
>> Epoch 79/100
acc: 95.369/95.397 loss: 0.083
>> Epoch 80/100
acc: 95.328/95.397 loss: 0.087
>> Epoch 81/100
acc: 95.543/95.543 loss: 0.084
>> Epoch 82/100
acc: 95.557/95.557 loss: 0.087
>> Epoch 83/100
acc: 95.463/95.557 loss: 0.084
>> Epoch 84/100
acc: 95.647/95.647 loss: 0.081
>> Epoch 85/100
acc: 95.731/95.731 loss: 0.081
>> Epoch 86/100
acc: 95.634/95.731 loss: 0.083
>> Epoch 87/100
acc: 95.399/95.731 loss: 0.085
>> Epoch 88/100
acc: 95.532/95.731 loss: 0.085
>> Epoch 89/100
acc: 95.469/95.731 loss: 0.084
>> Epoch 90/100
acc: 95.495/95.731 loss: 0.083
>> Epoch 91/100
acc: 95.661/95.731 loss: 0.082
>> Epoch 92/100
acc: 95.622/95.731 loss: 0.083
>> Epoch 93/100
acc: 95.698/95.731 loss: 0.081
>> Epoch 94/100
acc: 95.679/95.731 loss: 0.080
>> Epoch 95/100
acc: 95.614/95.731 loss: 0.083
>> Epoch 96/100
acc: 95.696/95.731 loss: 0.080
>> Epoch 97/100
acc: 95.780/95.780 loss: 0.077
>> Epoch 98/100
acc: 95.563/95.780 loss: 0.080
>> Epoch 99/100
acc: 95.822/95.822 loss: 0.079
>> Epoch 100/100
acc: 95.584/95.822 loss: 0.083
mg_catsiam_MobileNetV3_SOPClass_96_100_triplet_all_triplet_0.0001_256  saved.
mg_catsiam_emb_MobileNetV3_SOPClass_96_100_triplet_all_triplet_0.0001_256  saved.
== Validation ==

Accuracy (%): 95.82207489013672	 Error rate (%): 4.177925109863281
Finished. Total elapsed time (h:m:s): 7:41:23
Currently using GPU: 0
Creating dataset: SOPClass
create labels..
Creating model: mobilenet
MobileNet(
  (base_model): MobileNetV3(
    (features): Sequential(
      (0): ConvBNActivation(
        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
      (1): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (2): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (3): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (4): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (5): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (6): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (7): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (8): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (9): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (10): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (11): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (12): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (13): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (14): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (15): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (16): ConvBNActivation(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (classifier): Sequential(
      (0): Linear(in_features=960, out_features=1280, bias=True)
      (1): Hardswish()
      (2): Dropout(p=0.2, inplace=True)
      (3): Linear(in_features=1280, out_features=12, bias=True)
    )
  )
)
Linear(in_features=1280, out_features=256, bias=True)
file name: mg_catsiam_MobileNetV3_SOPClass_96_100_triplet_easy_triplet_0.0001_256
>> Epoch 1/100
acc: 64.582/64.582 loss: 0.933
>> Epoch 2/100
acc: 73.794/73.794 loss: 0.591
>> Epoch 3/100
acc: 76.602/76.602 loss: 0.501
>> Epoch 4/100
acc: 78.299/78.299 loss: 0.452
>> Epoch 5/100
acc: 79.913/79.913 loss: 0.400
>> Epoch 6/100
acc: 81.589/81.589 loss: 0.368
>> Epoch 7/100
acc: 82.447/82.447 loss: 0.339
>> Epoch 8/100
acc: 83.354/83.354 loss: 0.324
>> Epoch 9/100
acc: 84.214/84.214 loss: 0.305
>> Epoch 10/100
acc: 84.858/84.858 loss: 0.286
>> Epoch 11/100
acc: 85.565/85.565 loss: 0.270
>> Epoch 12/100
acc: 86.214/86.214 loss: 0.257
>> Epoch 13/100
acc: 87.008/87.008 loss: 0.245
>> Epoch 14/100
acc: 87.263/87.263 loss: 0.239
>> Epoch 15/100
acc: 87.899/87.899 loss: 0.226
>> Epoch 16/100
acc: 88.272/88.272 loss: 0.220
>> Epoch 17/100
acc: 88.457/88.457 loss: 0.212
>> Epoch 18/100
acc: 88.899/88.899 loss: 0.208
>> Epoch 19/100
acc: 89.384/89.384 loss: 0.200
>> Epoch 20/100
acc: 89.653/89.653 loss: 0.196
>> Epoch 21/100
acc: 90.496/90.496 loss: 0.178
>> Epoch 22/100
acc: 91.108/91.108 loss: 0.166
>> Epoch 23/100
acc: 91.157/91.157 loss: 0.162
>> Epoch 24/100
acc: 91.755/91.755 loss: 0.155
>> Epoch 25/100
acc: 91.590/91.755 loss: 0.155
>> Epoch 26/100
acc: 92.138/92.138 loss: 0.146
>> Epoch 27/100
acc: 92.109/92.138 loss: 0.147
>> Epoch 28/100
acc: 92.205/92.205 loss: 0.144
>> Epoch 29/100
acc: 92.081/92.205 loss: 0.147
>> Epoch 30/100
acc: 92.670/92.670 loss: 0.139
>> Epoch 31/100
acc: 92.492/92.670 loss: 0.139
>> Epoch 32/100
acc: 92.470/92.670 loss: 0.144
>> Epoch 33/100
acc: 92.823/92.823 loss: 0.134
>> Epoch 34/100
acc: 92.914/92.914 loss: 0.132
>> Epoch 35/100
acc: 92.917/92.917 loss: 0.131
>> Epoch 36/100
acc: 92.919/92.919 loss: 0.131
>> Epoch 37/100
acc: 93.085/93.085 loss: 0.132
>> Epoch 38/100
acc: 93.150/93.150 loss: 0.130
>> Epoch 39/100
acc: 93.120/93.150 loss: 0.130
>> Epoch 40/100
acc: 93.214/93.214 loss: 0.126
>> Epoch 41/100
acc: 93.941/93.941 loss: 0.112
>> Epoch 42/100
acc: 93.970/93.970 loss: 0.111
>> Epoch 43/100
acc: 94.108/94.108 loss: 0.112
>> Epoch 44/100
acc: 93.871/94.108 loss: 0.112
>> Epoch 45/100
acc: 94.307/94.307 loss: 0.109
>> Epoch 46/100
acc: 94.119/94.307 loss: 0.110
>> Epoch 47/100
acc: 94.328/94.328 loss: 0.109
>> Epoch 48/100
acc: 94.312/94.328 loss: 0.105
>> Epoch 49/100
acc: 94.287/94.328 loss: 0.110
>> Epoch 50/100
acc: 94.349/94.349 loss: 0.104
>> Epoch 51/100
acc: 94.437/94.437 loss: 0.107
>> Epoch 52/100
acc: 94.484/94.484 loss: 0.103
>> Epoch 53/100
acc: 94.554/94.554 loss: 0.100
>> Epoch 54/100
acc: 94.534/94.554 loss: 0.101
>> Epoch 55/100
acc: 94.516/94.554 loss: 0.102
>> Epoch 56/100
acc: 94.675/94.675 loss: 0.100
>> Epoch 57/100
acc: 94.559/94.675 loss: 0.101
>> Epoch 58/100
acc: 94.554/94.675 loss: 0.099
>> Epoch 59/100
acc: 94.699/94.699 loss: 0.101
>> Epoch 60/100
acc: 94.857/94.857 loss: 0.099
>> Epoch 61/100
acc: 94.957/94.957 loss: 0.095
>> Epoch 62/100
acc: 95.046/95.046 loss: 0.096
>> Epoch 63/100
acc: 95.157/95.157 loss: 0.091
>> Epoch 64/100
acc: 95.090/95.157 loss: 0.092
>> Epoch 65/100
acc: 95.078/95.157 loss: 0.092
>> Epoch 66/100
acc: 95.182/95.182 loss: 0.091
>> Epoch 67/100
acc: 95.318/95.318 loss: 0.092
>> Epoch 68/100
acc: 95.102/95.318 loss: 0.091
>> Epoch 69/100
acc: 95.197/95.318 loss: 0.089
>> Epoch 70/100
acc: 95.419/95.419 loss: 0.088
>> Epoch 71/100
acc: 95.260/95.419 loss: 0.090
>> Epoch 72/100
acc: 95.273/95.419 loss: 0.091
>> Epoch 73/100
acc: 95.291/95.419 loss: 0.090
>> Epoch 74/100
acc: 95.303/95.419 loss: 0.087
>> Epoch 75/100
acc: 95.248/95.419 loss: 0.087
>> Epoch 76/100
acc: 95.417/95.419 loss: 0.087
>> Epoch 77/100
acc: 95.347/95.419 loss: 0.089
>> Epoch 78/100
acc: 95.288/95.419 loss: 0.088
>> Epoch 79/100
acc: 95.335/95.419 loss: 0.083
>> Epoch 80/100
acc: 95.352/95.419 loss: 0.087
>> Epoch 81/100
acc: 95.597/95.597 loss: 0.084
>> Epoch 82/100
acc: 95.543/95.597 loss: 0.087
>> Epoch 83/100
acc: 95.523/95.597 loss: 0.083
>> Epoch 84/100
acc: 95.595/95.597 loss: 0.081
>> Epoch 85/100
acc: 95.656/95.656 loss: 0.081
>> Epoch 86/100
acc: 95.599/95.656 loss: 0.083
>> Epoch 87/100
acc: 95.434/95.656 loss: 0.085
>> Epoch 88/100
acc: 95.530/95.656 loss: 0.085
>> Epoch 89/100
acc: 95.503/95.656 loss: 0.084
>> Epoch 90/100
acc: 95.604/95.656 loss: 0.083
>> Epoch 91/100
acc: 95.666/95.666 loss: 0.083
>> Epoch 92/100
acc: 95.577/95.666 loss: 0.083
>> Epoch 93/100
acc: 95.651/95.666 loss: 0.082
>> Epoch 94/100
acc: 95.639/95.666 loss: 0.081
>> Epoch 95/100
acc: 95.572/95.666 loss: 0.082
>> Epoch 96/100
acc: 95.705/95.705 loss: 0.080
>> Epoch 97/100
acc: 95.740/95.740 loss: 0.078
>> Epoch 98/100
acc: 95.569/95.740 loss: 0.080
>> Epoch 99/100
acc: 95.815/95.815 loss: 0.080
>> Epoch 100/100
acc: 95.552/95.815 loss: 0.083
mg_catsiam_MobileNetV3_SOPClass_96_100_triplet_easy_triplet_0.0001_256  saved.
mg_catsiam_emb_MobileNetV3_SOPClass_96_100_triplet_easy_triplet_0.0001_256  saved.
== Validation ==

Accuracy (%): 95.81535339355469	 Error rate (%): 4.1846466064453125
Finished. Total elapsed time (h:m:s): 7:41:07
Currently using GPU: 0
Creating dataset: SOPClass
create labels..
Creating model: mobilenet
MobileNet(
  (base_model): MobileNetV3(
    (features): Sequential(
      (0): ConvBNActivation(
        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
      (1): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (2): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (3): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (4): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (5): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (6): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (7): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (8): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (9): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (10): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (11): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (12): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (13): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (14): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (15): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (16): ConvBNActivation(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (classifier): Sequential(
      (0): Linear(in_features=960, out_features=1280, bias=True)
      (1): Hardswish()
      (2): Dropout(p=0.2, inplace=True)
      (3): Linear(in_features=1280, out_features=12, bias=True)
    )
  )
)
Linear(in_features=1280, out_features=256, bias=True)
file name: mg_catsiam_MobileNetV3_SOPClass_96_100_triplet_semi_triplet_0.0001_256
>> Epoch 1/100
acc: 64.570/64.570 loss: 0.933
>> Epoch 2/100
acc: 73.814/73.814 loss: 0.591
>> Epoch 3/100
acc: 76.539/76.539 loss: 0.501
>> Epoch 4/100
acc: 78.286/78.286 loss: 0.452
>> Epoch 5/100
acc: 79.950/79.950 loss: 0.400
>> Epoch 6/100
acc: 81.562/81.562 loss: 0.368
>> Epoch 7/100
acc: 82.318/82.318 loss: 0.338
>> Epoch 8/100
acc: 83.423/83.423 loss: 0.323
>> Epoch 9/100
acc: 84.203/84.203 loss: 0.304
>> Epoch 10/100
acc: 84.847/84.847 loss: 0.287
>> Epoch 11/100
acc: 85.470/85.470 loss: 0.270
>> Epoch 12/100
acc: 86.244/86.244 loss: 0.256
>> Epoch 13/100
acc: 86.991/86.991 loss: 0.245
>> Epoch 14/100
acc: 87.357/87.357 loss: 0.240
>> Epoch 15/100
acc: 87.958/87.958 loss: 0.225
>> Epoch 16/100
acc: 88.252/88.252 loss: 0.220
>> Epoch 17/100
acc: 88.479/88.479 loss: 0.214
>> Epoch 18/100
acc: 88.853/88.853 loss: 0.209
>> Epoch 19/100
acc: 89.394/89.394 loss: 0.199
>> Epoch 20/100
acc: 89.589/89.589 loss: 0.196
>> Epoch 21/100
acc: 90.457/90.457 loss: 0.178
>> Epoch 22/100
acc: 91.145/91.145 loss: 0.166
>> Epoch 23/100
acc: 91.236/91.236 loss: 0.162
>> Epoch 24/100
acc: 91.735/91.735 loss: 0.155
>> Epoch 25/100
acc: 91.599/91.735 loss: 0.154
>> Epoch 26/100
acc: 91.990/91.990 loss: 0.147
>> Epoch 27/100
acc: 92.131/92.131 loss: 0.147
>> Epoch 28/100
acc: 92.235/92.235 loss: 0.144
>> Epoch 29/100
acc: 92.121/92.235 loss: 0.147
>> Epoch 30/100
acc: 92.680/92.680 loss: 0.139
>> Epoch 31/100
acc: 92.499/92.680 loss: 0.139
>> Epoch 32/100
acc: 92.485/92.680 loss: 0.143
>> Epoch 33/100
acc: 92.764/92.764 loss: 0.134
>> Epoch 34/100
acc: 93.006/93.006 loss: 0.131
>> Epoch 35/100
acc: 92.941/93.006 loss: 0.131
>> Epoch 36/100
acc: 92.929/93.006 loss: 0.131
>> Epoch 37/100
acc: 93.014/93.014 loss: 0.132
>> Epoch 38/100
acc: 93.187/93.187 loss: 0.129
>> Epoch 39/100
acc: 93.187/93.187 loss: 0.130
>> Epoch 40/100
acc: 93.253/93.253 loss: 0.125
>> Epoch 41/100
acc: 93.982/93.982 loss: 0.112
>> Epoch 42/100
acc: 93.983/93.983 loss: 0.110
>> Epoch 43/100
acc: 94.210/94.210 loss: 0.111
>> Epoch 44/100
acc: 93.920/94.210 loss: 0.111
>> Epoch 45/100
acc: 94.240/94.240 loss: 0.109
>> Epoch 46/100
acc: 94.185/94.240 loss: 0.110
>> Epoch 47/100
acc: 94.312/94.312 loss: 0.109
>> Epoch 48/100
acc: 94.282/94.312 loss: 0.105
>> Epoch 49/100
acc: 94.225/94.312 loss: 0.110
>> Epoch 50/100
acc: 94.381/94.381 loss: 0.105
>> Epoch 51/100
acc: 94.459/94.459 loss: 0.106
>> Epoch 52/100
acc: 94.546/94.546 loss: 0.103
>> Epoch 53/100
acc: 94.605/94.605 loss: 0.099
>> Epoch 54/100
acc: 94.593/94.605 loss: 0.101
>> Epoch 55/100
acc: 94.502/94.605 loss: 0.103
>> Epoch 56/100
acc: 94.663/94.663 loss: 0.100
>> Epoch 57/100
acc: 94.611/94.663 loss: 0.101
>> Epoch 58/100
acc: 94.571/94.663 loss: 0.100
>> Epoch 59/100
acc: 94.702/94.702 loss: 0.101
>> Epoch 60/100
acc: 94.878/94.878 loss: 0.098
>> Epoch 61/100
acc: 94.979/94.979 loss: 0.095
>> Epoch 62/100
acc: 95.008/95.008 loss: 0.096
>> Epoch 63/100
acc: 95.118/95.118 loss: 0.091
>> Epoch 64/100
acc: 95.038/95.118 loss: 0.092
>> Epoch 65/100
acc: 95.097/95.118 loss: 0.092
>> Epoch 66/100
acc: 95.228/95.228 loss: 0.090
>> Epoch 67/100
acc: 95.302/95.302 loss: 0.092
>> Epoch 68/100
acc: 95.103/95.302 loss: 0.090
>> Epoch 69/100
acc: 95.219/95.302 loss: 0.089
>> Epoch 70/100
acc: 95.396/95.396 loss: 0.088
>> Epoch 71/100
acc: 95.239/95.396 loss: 0.090
>> Epoch 72/100
acc: 95.258/95.396 loss: 0.091
>> Epoch 73/100
acc: 95.258/95.396 loss: 0.090
>> Epoch 74/100
acc: 95.290/95.396 loss: 0.087
>> Epoch 75/100
acc: 95.268/95.396 loss: 0.087
>> Epoch 76/100
acc: 95.379/95.396 loss: 0.087
>> Epoch 77/100
acc: 95.404/95.404 loss: 0.088
>> Epoch 78/100
acc: 95.199/95.404 loss: 0.088
>> Epoch 79/100
acc: 95.404/95.404 loss: 0.083
>> Epoch 80/100
acc: 95.349/95.404 loss: 0.086
>> Epoch 81/100
acc: 95.565/95.565 loss: 0.083
>> Epoch 82/100
acc: 95.610/95.610 loss: 0.087
>> Epoch 83/100
acc: 95.525/95.610 loss: 0.083
>> Epoch 84/100
acc: 95.587/95.610 loss: 0.081
>> Epoch 85/100
acc: 95.706/95.706 loss: 0.081
>> Epoch 86/100
acc: 95.627/95.706 loss: 0.082
>> Epoch 87/100
acc: 95.431/95.706 loss: 0.085
>> Epoch 88/100
acc: 95.530/95.706 loss: 0.085
>> Epoch 89/100
acc: 95.478/95.706 loss: 0.084
>> Epoch 90/100
acc: 95.498/95.706 loss: 0.084
>> Epoch 91/100
acc: 95.631/95.706 loss: 0.083
>> Epoch 92/100
acc: 95.570/95.706 loss: 0.083
>> Epoch 93/100
acc: 95.730/95.730 loss: 0.081
>> Epoch 94/100
acc: 95.671/95.730 loss: 0.081
>> Epoch 95/100
acc: 95.600/95.730 loss: 0.083
>> Epoch 96/100
acc: 95.668/95.730 loss: 0.080
>> Epoch 97/100
acc: 95.785/95.785 loss: 0.077
>> Epoch 98/100
acc: 95.686/95.785 loss: 0.080
>> Epoch 99/100
acc: 95.836/95.836 loss: 0.080
>> Epoch 100/100
acc: 95.535/95.836 loss: 0.083
mg_catsiam_MobileNetV3_SOPClass_96_100_triplet_semi_triplet_0.0001_256  saved.
mg_catsiam_emb_MobileNetV3_SOPClass_96_100_triplet_semi_triplet_0.0001_256  saved.
== Validation ==

Accuracy (%): 95.83550262451172	 Error rate (%): 4.164497375488281
Finished. Total elapsed time (h:m:s): 7:41:27
Currently using GPU: 0
Creating dataset: SOPClass
create labels..
Creating model: mobilenet
MobileNet(
  (base_model): MobileNetV3(
    (features): Sequential(
      (0): ConvBNActivation(
        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
      (1): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (2): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (3): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (4): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (5): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (6): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (7): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (8): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (9): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (10): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (11): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (12): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (13): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (14): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (15): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (16): ConvBNActivation(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (classifier): Sequential(
      (0): Linear(in_features=960, out_features=1280, bias=True)
      (1): Hardswish()
      (2): Dropout(p=0.2, inplace=True)
      (3): Linear(in_features=1280, out_features=12, bias=True)
    )
  )
)
Linear(in_features=1280, out_features=256, bias=True)
file name: mg_catsiam_MobileNetV3_SOPClass_96_100_triplet_hard_triplet_0.0001_256
>> Epoch 1/100
acc: 64.597/64.597 loss: 0.933
>> Epoch 2/100
acc: 73.787/73.787 loss: 0.591
>> Epoch 3/100
acc: 76.466/76.466 loss: 0.501
>> Epoch 4/100
acc: 78.410/78.410 loss: 0.452
>> Epoch 5/100
acc: 79.968/79.968 loss: 0.400
>> Epoch 6/100
acc: 81.513/81.513 loss: 0.368
>> Epoch 7/100
acc: 82.376/82.376 loss: 0.339
>> Epoch 8/100
acc: 83.431/83.431 loss: 0.323
>> Epoch 9/100
acc: 84.219/84.219 loss: 0.305
>> Epoch 10/100
acc: 84.892/84.892 loss: 0.286
>> Epoch 11/100
acc: 85.523/85.523 loss: 0.271
>> Epoch 12/100
acc: 86.277/86.277 loss: 0.257
>> Epoch 13/100
acc: 86.949/86.949 loss: 0.245
>> Epoch 14/100
acc: 87.330/87.330 loss: 0.240
>> Epoch 15/100
acc: 87.889/87.889 loss: 0.225
>> Epoch 16/100
acc: 88.240/88.240 loss: 0.220
>> Epoch 17/100
acc: 88.611/88.611 loss: 0.213
>> Epoch 18/100
acc: 88.915/88.915 loss: 0.208
>> Epoch 19/100
acc: 89.392/89.392 loss: 0.199
>> Epoch 20/100
acc: 89.730/89.730 loss: 0.196
>> Epoch 21/100
acc: 90.506/90.506 loss: 0.178
>> Epoch 22/100
acc: 91.162/91.162 loss: 0.165
>> Epoch 23/100
acc: 91.216/91.216 loss: 0.162
>> Epoch 24/100
acc: 91.726/91.726 loss: 0.155
>> Epoch 25/100
acc: 91.553/91.726 loss: 0.155
>> Epoch 26/100
acc: 92.067/92.067 loss: 0.147
>> Epoch 27/100
acc: 92.103/92.103 loss: 0.147
>> Epoch 28/100
acc: 92.190/92.190 loss: 0.144
>> Epoch 29/100
acc: 92.116/92.190 loss: 0.147
>> Epoch 30/100
acc: 92.630/92.630 loss: 0.139
>> Epoch 31/100
acc: 92.485/92.630 loss: 0.139
>> Epoch 32/100
acc: 92.450/92.630 loss: 0.143
>> Epoch 33/100
acc: 92.774/92.774 loss: 0.134
>> Epoch 34/100
acc: 92.974/92.974 loss: 0.132
>> Epoch 35/100
acc: 92.935/92.974 loss: 0.131
>> Epoch 36/100
acc: 92.862/92.974 loss: 0.131
>> Epoch 37/100
acc: 93.090/93.090 loss: 0.132
>> Epoch 38/100
acc: 93.239/93.239 loss: 0.129
>> Epoch 39/100
acc: 93.125/93.239 loss: 0.130
>> Epoch 40/100
acc: 93.223/93.239 loss: 0.126
>> Epoch 41/100
acc: 94.024/94.024 loss: 0.112
>> Epoch 42/100
acc: 94.012/94.024 loss: 0.111
>> Epoch 43/100
acc: 94.109/94.109 loss: 0.112
>> Epoch 44/100
acc: 93.901/94.109 loss: 0.111
>> Epoch 45/100
acc: 94.225/94.225 loss: 0.108
>> Epoch 46/100
acc: 94.158/94.225 loss: 0.111
>> Epoch 47/100
acc: 94.291/94.291 loss: 0.109
>> Epoch 48/100
acc: 94.370/94.370 loss: 0.104
>> Epoch 49/100
acc: 94.264/94.370 loss: 0.109
>> Epoch 50/100
acc: 94.366/94.370 loss: 0.105
>> Epoch 51/100
acc: 94.437/94.437 loss: 0.106
>> Epoch 52/100
acc: 94.465/94.465 loss: 0.103
>> Epoch 53/100
acc: 94.569/94.569 loss: 0.099
>> Epoch 54/100
acc: 94.595/94.595 loss: 0.101
>> Epoch 55/100
acc: 94.596/94.596 loss: 0.103
>> Epoch 56/100
acc: 94.581/94.596 loss: 0.100
>> Epoch 57/100
acc: 94.568/94.596 loss: 0.102
>> Epoch 58/100
acc: 94.564/94.596 loss: 0.099
>> Epoch 59/100
acc: 94.778/94.778 loss: 0.101
>> Epoch 60/100
acc: 94.803/94.803 loss: 0.099
>> Epoch 61/100
acc: 94.954/94.954 loss: 0.094
>> Epoch 62/100
acc: 95.033/95.033 loss: 0.096
>> Epoch 63/100
acc: 95.213/95.213 loss: 0.091
>> Epoch 64/100
acc: 95.031/95.213 loss: 0.092
>> Epoch 65/100
acc: 95.113/95.213 loss: 0.093
>> Epoch 66/100
acc: 95.140/95.213 loss: 0.091
>> Epoch 67/100
acc: 95.271/95.271 loss: 0.091
>> Epoch 68/100
acc: 95.073/95.271 loss: 0.091
>> Epoch 69/100
acc: 95.244/95.271 loss: 0.088
>> Epoch 70/100
acc: 95.426/95.426 loss: 0.088
>> Epoch 71/100
acc: 95.187/95.426 loss: 0.090
>> Epoch 72/100
acc: 95.258/95.426 loss: 0.091
>> Epoch 73/100
acc: 95.204/95.426 loss: 0.091
>> Epoch 74/100
acc: 95.241/95.426 loss: 0.088
>> Epoch 75/100
acc: 95.226/95.426 loss: 0.087
>> Epoch 76/100
acc: 95.421/95.426 loss: 0.087
>> Epoch 77/100
acc: 95.364/95.426 loss: 0.088
>> Epoch 78/100
acc: 95.288/95.426 loss: 0.088
>> Epoch 79/100
acc: 95.438/95.438 loss: 0.083
>> Epoch 80/100
acc: 95.385/95.438 loss: 0.086
>> Epoch 81/100
acc: 95.574/95.574 loss: 0.083
>> Epoch 82/100
acc: 95.533/95.574 loss: 0.087
>> Epoch 83/100
acc: 95.468/95.574 loss: 0.084
>> Epoch 84/100
acc: 95.693/95.693 loss: 0.081
>> Epoch 85/100
acc: 95.689/95.693 loss: 0.081
>> Epoch 86/100
acc: 95.656/95.693 loss: 0.082
>> Epoch 87/100
acc: 95.471/95.693 loss: 0.085
>> Epoch 88/100
acc: 95.454/95.693 loss: 0.086
>> Epoch 89/100
acc: 95.511/95.693 loss: 0.084
>> Epoch 90/100
acc: 95.511/95.693 loss: 0.083
>> Epoch 91/100
acc: 95.651/95.693 loss: 0.082
>> Epoch 92/100
acc: 95.600/95.693 loss: 0.083
>> Epoch 93/100
acc: 95.701/95.701 loss: 0.081
>> Epoch 94/100
acc: 95.639/95.701 loss: 0.080
>> Epoch 95/100
acc: 95.585/95.701 loss: 0.083
>> Epoch 96/100
acc: 95.706/95.706 loss: 0.080
>> Epoch 97/100
acc: 95.755/95.755 loss: 0.077
>> Epoch 98/100
acc: 95.592/95.755 loss: 0.080
>> Epoch 99/100
acc: 95.809/95.809 loss: 0.080
>> Epoch 100/100
acc: 95.587/95.809 loss: 0.083
mg_catsiam_MobileNetV3_SOPClass_96_100_triplet_hard_triplet_0.0001_256  saved.
mg_catsiam_emb_MobileNetV3_SOPClass_96_100_triplet_hard_triplet_0.0001_256  saved.
== Validation ==

Accuracy (%): 95.80863952636719	 Error rate (%): 4.1913604736328125
Finished. Total elapsed time (h:m:s): 7:40:12
