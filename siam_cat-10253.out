Currently using GPU: 0
Creating dataset: custShopClass
Creating model: mobilenet
MobileNet(
  (base_model): MobileNetV3(
    (features): Sequential(
      (0): ConvBNActivation(
        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
      (1): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (2): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (3): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (4): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (5): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (6): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (7): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (8): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (9): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (10): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (11): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (12): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (13): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (14): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (15): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (16): ConvBNActivation(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (classifier): Sequential(
      (0): Linear(in_features=960, out_features=1280, bias=True)
      (1): Hardswish()
      (2): Dropout(p=0.2, inplace=True)
      (3): Linear(in_features=1280, out_features=23, bias=True)
    )
  )
)
Linear(in_features=1280, out_features=128, bias=True)
file name: mg_catsiam_MobileNetV3_custShopClass_96_100_pairmargin_pair_0.0001_128
>> Epoch 1/100
acc: 48.526/48.526 loss: 0.761
>> Epoch 2/100
acc: 54.569/54.569 loss: 0.573
>> Epoch 3/100
acc: 57.642/57.642 loss: 0.512
>> Epoch 4/100
acc: 59.858/59.858 loss: 0.472
>> Epoch 5/100
acc: 61.536/61.536 loss: 0.447
>> Epoch 6/100
acc: 62.979/62.979 loss: 0.424
>> Epoch 7/100
acc: 64.137/64.137 loss: 0.404
>> Epoch 8/100
acc: 65.189/65.189 loss: 0.389
>> Epoch 9/100
acc: 66.225/66.225 loss: 0.372
>> Epoch 10/100
acc: 66.767/66.767 loss: 0.366
>> Epoch 11/100
acc: 67.567/67.567 loss: 0.355
>> Epoch 12/100
acc: 68.192/68.192 loss: 0.345
>> Epoch 13/100
acc: 68.721/68.721 loss: 0.339
>> Epoch 14/100
acc: 69.497/69.497 loss: 0.329
>> Epoch 15/100
acc: 69.679/69.679 loss: 0.325
>> Epoch 16/100
acc: 70.061/70.061 loss: 0.320
>> Epoch 17/100
acc: 70.676/70.676 loss: 0.311
>> Epoch 18/100
acc: 70.941/70.941 loss: 0.306
>> Epoch 19/100
acc: 71.245/71.245 loss: 0.304
>> Epoch 20/100
acc: 71.630/71.630 loss: 0.296
>> Epoch 21/100
acc: 73.853/73.853 loss: 0.267
>> Epoch 22/100
acc: 74.525/74.525 loss: 0.256
>> Epoch 23/100
acc: 74.964/74.964 loss: 0.251
>> Epoch 24/100
acc: 75.207/75.207 loss: 0.248
>> Epoch 25/100
acc: 75.499/75.499 loss: 0.245
>> Epoch 26/100
acc: 75.565/75.565 loss: 0.244
>> Epoch 27/100
acc: 75.933/75.933 loss: 0.239
>> Epoch 28/100
acc: 76.300/76.300 loss: 0.236
>> Epoch 29/100
acc: 76.254/76.300 loss: 0.235
>> Epoch 30/100
acc: 76.706/76.706 loss: 0.233
>> Epoch 31/100
acc: 76.766/76.766 loss: 0.230
>> Epoch 32/100
acc: 76.828/76.828 loss: 0.227
>> Epoch 33/100
acc: 77.207/77.207 loss: 0.224
>> Epoch 34/100
acc: 77.162/77.207 loss: 0.225
>> Epoch 35/100
acc: 77.513/77.513 loss: 0.222
>> Epoch 36/100
acc: 77.787/77.787 loss: 0.220
>> Epoch 37/100
acc: 77.807/77.807 loss: 0.218
>> Epoch 38/100
acc: 77.979/77.979 loss: 0.215
>> Epoch 39/100
acc: 78.044/78.044 loss: 0.216
>> Epoch 40/100
acc: 78.216/78.216 loss: 0.214
>> Epoch 41/100
acc: 79.767/79.767 loss: 0.197
>> Epoch 42/100
acc: 80.291/80.291 loss: 0.190
>> Epoch 43/100
acc: 80.255/80.291 loss: 0.190
>> Epoch 44/100
acc: 80.434/80.434 loss: 0.189
>> Epoch 45/100
acc: 80.576/80.576 loss: 0.187
>> Epoch 46/100
acc: 80.651/80.651 loss: 0.187
>> Epoch 47/100
acc: 80.718/80.718 loss: 0.185
>> Epoch 48/100
acc: 81.000/81.000 loss: 0.182
>> Epoch 49/100
acc: 81.051/81.051 loss: 0.183
>> Epoch 50/100
acc: 81.158/81.158 loss: 0.181
>> Epoch 51/100
acc: 81.250/81.250 loss: 0.181
>> Epoch 52/100
acc: 81.374/81.374 loss: 0.177
>> Epoch 53/100
acc: 81.386/81.386 loss: 0.180
>> Epoch 54/100
acc: 81.543/81.543 loss: 0.177
>> Epoch 55/100
acc: 81.490/81.543 loss: 0.178
>> Epoch 56/100
acc: 81.755/81.755 loss: 0.176
>> Epoch 57/100
acc: 81.809/81.809 loss: 0.176
>> Epoch 58/100
acc: 81.943/81.943 loss: 0.173
>> Epoch 59/100
acc: 81.740/81.943 loss: 0.174
>> Epoch 60/100
acc: 81.968/81.968 loss: 0.174
>> Epoch 61/100
