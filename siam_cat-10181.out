Currently using GPU: 0
Creating dataset: SOPClass
create labels..
Creating model: mobilenet
MobileNet(
  (base_model): MobileNetV3(
    (features): Sequential(
      (0): ConvBNActivation(
        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
      (1): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (2): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (3): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): ConvBNActivation(
            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (4): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (5): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (6): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): ConvBNActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (7): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (8): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)
            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (9): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (10): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): ConvBNActivation(
            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (11): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (12): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (13): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (14): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (15): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): ConvBNActivation(
            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))
            (relu): ReLU(inplace=True)
            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))
          )
          (3): ConvBNActivation(
            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (16): ConvBNActivation(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (classifier): Sequential(
      (0): Linear(in_features=960, out_features=1280, bias=True)
      (1): Hardswish()
      (2): Dropout(p=0.2, inplace=True)
      (3): Linear(in_features=1280, out_features=12, bias=True)
    )
  )
)
Linear(in_features=1280, out_features=512, bias=True)
file name: mg_catsiam_MobileNetV3_SOPClass_64_50_triplet_all_triplet_0.0001_512
>> Epoch 1/50
acc: 65.425/65.425 loss: 0.898
>> Epoch 2/50
acc: 73.824/73.824 loss: 0.590
>> Epoch 3/50
acc: 76.440/76.440 loss: 0.503
>> Epoch 4/50
acc: 78.531/78.531 loss: 0.449
>> Epoch 5/50
acc: 80.306/80.306 loss: 0.402
>> Epoch 6/50
acc: 81.109/81.109 loss: 0.374
>> Epoch 7/50
acc: 82.247/82.247 loss: 0.344
>> Epoch 8/50
acc: 83.285/83.285 loss: 0.324
>> Epoch 9/50
acc: 83.957/83.957 loss: 0.310
>> Epoch 10/50
acc: 84.828/84.828 loss: 0.288
>> Epoch 11/50
acc: 85.520/85.520 loss: 0.278
>> Epoch 12/50
acc: 85.881/85.881 loss: 0.271
>> Epoch 13/50
acc: 86.522/86.522 loss: 0.250
>> Epoch 14/50
acc: 87.132/87.132 loss: 0.241
>> Epoch 15/50
acc: 87.560/87.560 loss: 0.236
>> Epoch 16/50
acc: 87.794/87.794 loss: 0.229
>> Epoch 17/50
acc: 88.082/88.082 loss: 0.223
>> Epoch 18/50
acc: 88.398/88.398 loss: 0.216
>> Epoch 19/50
acc: 88.801/88.801 loss: 0.211
>> Epoch 20/50
acc: 89.236/89.236 loss: 0.208
>> Epoch 21/50
acc: 90.799/90.799 loss: 0.178
>> Epoch 22/50
acc: 90.979/90.979 loss: 0.164
>> Epoch 23/50
acc: 91.243/91.243 loss: 0.164
>> Epoch 24/50
acc: 91.517/91.517 loss: 0.156
>> Epoch 25/50
acc: 91.407/91.517 loss: 0.159
>> Epoch 26/50
acc: 91.901/91.901 loss: 0.151
>> Epoch 27/50
acc: 91.837/91.901 loss: 0.150
>> Epoch 28/50
acc: 91.992/91.992 loss: 0.150
>> Epoch 29/50
acc: 92.084/92.084 loss: 0.148
>> Epoch 30/50
acc: 92.218/92.218 loss: 0.145
>> Epoch 31/50
acc: 92.361/92.361 loss: 0.143
>> Epoch 32/50
acc: 92.323/92.361 loss: 0.142
>> Epoch 33/50
acc: 92.467/92.467 loss: 0.140
>> Epoch 34/50
acc: 92.532/92.532 loss: 0.141
>> Epoch 35/50
acc: 92.702/92.702 loss: 0.141
>> Epoch 36/50
acc: 92.793/92.793 loss: 0.133
>> Epoch 37/50
acc: 92.742/92.793 loss: 0.134
>> Epoch 38/50
acc: 92.830/92.830 loss: 0.131
>> Epoch 39/50
acc: 93.035/93.035 loss: 0.129
>> Epoch 40/50
acc: 92.930/93.035 loss: 0.134
>> Epoch 41/50
acc: 93.597/93.597 loss: 0.116
>> Epoch 42/50
acc: 93.982/93.982 loss: 0.112
>> Epoch 43/50
acc: 93.990/93.990 loss: 0.113
>> Epoch 44/50
acc: 94.052/94.052 loss: 0.110
>> Epoch 45/50
acc: 94.052/94.052 loss: 0.110
>> Epoch 46/50
acc: 94.150/94.150 loss: 0.110
>> Epoch 47/50
acc: 94.234/94.234 loss: 0.108
>> Epoch 48/50
acc: 94.175/94.234 loss: 0.107
>> Epoch 49/50
acc: 94.410/94.410 loss: 0.105
>> Epoch 50/50
acc: 94.257/94.410 loss: 0.107
mg_catsiam_MobileNetV3_SOPClass_64_50_triplet_all_triplet_0.0001_512  saved.
mg_catsiam_emb_MobileNetV3_SOPClass_64_50_triplet_all_triplet_0.0001_512  saved.
== Validation ==

Accuracy (%): 94.40983581542969	 Error rate (%): 5.5901641845703125
Finished. Total elapsed time (h:m:s): 4:05:33
